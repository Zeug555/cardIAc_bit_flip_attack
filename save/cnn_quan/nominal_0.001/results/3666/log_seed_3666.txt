save path : ./save/cnn_quan/nominal_0.001/results/3666
{'data_path': './dataset', 'arch': 'cnn_quan', 'dataset': 'mit-bih', 'epochs': 20, 'start_epoch': 0, 'attack_sample_size': 128, 'test_batch_size': 128, 'optimizer': 'Adam', 'schedule': [25, 40], 'gammas': [0.1, 0.1], 'workers': 4, 'ngpu': 0, 'gpu_id': 0, 'print_freq': 100, 'decay': 0.0003, 'momentum': 0.9, 'limit_layer': -1, 'randbet_coeff': 10, 'k_top': 20, 'randbet': False, 'clipping_coeff': 0.0, 'learning_rate': 0.001, 'manualSeed': 3666, 'save_path': './save/cnn_quan/nominal_0.001/results/3666', 'enable_bfa': True, 'resume': './save/cnn_quan/nominal_0.001/model_best.pth.tar', 'quan_bitwidth': None, 'reset_weight': True, 'evaluate': True, 'n_iter': 30, 'fine_tune': True, 'model_only': False, 'random_bfa': False, 'use_cuda': False}
Random Seed: 3666
python version : 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]
torch  version : 2.5.1+cpu
cudnn  version : None
=> creating model 'cnn_quan'
=> network :
 cnn_bones(
  (conv1): quan_Conv1d(1, 4, kernel_size=(21,), stride=(1,))
  (pool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (conv2): quan_Conv1d(4, 4, kernel_size=(21,), stride=(1,))
  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fc1): quan_Linear(in_features=44, out_features=32, bias=True)
  (fc2): quan_Linear(in_features=32, out_features=5, bias=True)
)
=> loading checkpoint './save/cnn_quan/nominal_0.001/model_best.pth.tar'
=> loaded checkpoint './save/cnn_quan/nominal_0.001/model_best.pth.tar' (epoch 0)
  **Test** Prec@1 89.500 Prec@5 100.000 Error@1 10.500
k_top=20
Attack_sample=50
************** ATTACK iteration *****************
Iteration: [001/030]   Attack Time 0.103 (0.103)  [2024-11-25 18:54:31]
loss before attack: 0.1506
loss after attack: 0.7259
bit flips: 1
hamming_dist: 0
  **Test** Prec@1 76.740 Prec@5 100.000 Error@1 23.260
iteration Time 0.150 (0.150)
************** ATTACK iteration *****************
Iteration: [002/030]   Attack Time 0.010 (0.056)  [2024-11-25 18:54:32]
loss before attack: 0.7259
loss after attack: 1.6674
bit flips: 2
hamming_dist: 1
  **Test** Prec@1 61.260 Prec@5 100.000 Error@1 38.740
iteration Time 0.134 (0.142)
************** ATTACK iteration *****************
Iteration: [003/030]   Attack Time 0.010 (0.041)  [2024-11-25 18:54:32]
loss before attack: 1.6674
loss after attack: 2.8982
bit flips: 3
hamming_dist: 2
  **Test** Prec@1 45.620 Prec@5 100.000 Error@1 54.380
iteration Time 0.135 (0.140)
************** ATTACK iteration *****************
Iteration: [004/030]   Attack Time 0.009 (0.033)  [2024-11-25 18:54:32]
loss before attack: 2.8982
loss after attack: 4.5140
bit flips: 4
hamming_dist: 2
  **Test** Prec@1 45.500 Prec@5 100.000 Error@1 54.500
iteration Time 0.135 (0.138)
************** ATTACK iteration *****************
Iteration: [005/030]   Attack Time 0.009 (0.028)  [2024-11-25 18:54:32]
loss before attack: 4.5140
loss after attack: 6.0631
bit flips: 5
hamming_dist: 2
  **Test** Prec@1 44.660 Prec@5 100.000 Error@1 55.340
iteration Time 0.145 (0.140)
************** ATTACK iteration *****************
Iteration: [006/030]   Attack Time 0.011 (0.025)  [2024-11-25 18:54:32]
loss before attack: 6.0631
loss after attack: 7.6763
bit flips: 6
hamming_dist: 2
  **Test** Prec@1 44.260 Prec@5 100.000 Error@1 55.740
iteration Time 0.139 (0.140)
************** ATTACK iteration *****************
Iteration: [007/030]   Attack Time 0.010 (0.023)  [2024-11-25 18:54:32]
loss before attack: 7.6763
loss after attack: 9.4051
bit flips: 7
hamming_dist: 3
  **Test** Prec@1 34.180 Prec@5 100.000 Error@1 65.820
iteration Time 0.134 (0.139)
************** ATTACK iteration *****************
Iteration: [008/030]   Attack Time 0.009 (0.021)  [2024-11-25 18:54:32]
loss before attack: 9.4051
loss after attack: 11.2840
bit flips: 8
hamming_dist: 4
  **Test** Prec@1 27.940 Prec@5 100.000 Error@1 72.060
iteration Time 0.151 (0.140)
************** ATTACK iteration *****************
Iteration: [009/030]   Attack Time 0.010 (0.020)  [2024-11-25 18:54:33]
loss before attack: 11.2840
loss after attack: 13.3270
bit flips: 9
hamming_dist: 4
  **Test** Prec@1 27.840 Prec@5 100.000 Error@1 72.160
iteration Time 0.135 (0.140)
************** ATTACK iteration *****************
Iteration: [010/030]   Attack Time 0.009 (0.019)  [2024-11-25 18:54:33]
loss before attack: 13.3270
loss after attack: 15.4882
bit flips: 10
hamming_dist: 5
  **Test** Prec@1 23.040 Prec@5 100.000 Error@1 76.960
iteration Time 0.140 (0.140)
************** ATTACK iteration *****************
Iteration: [011/030]   Attack Time 0.011 (0.018)  [2024-11-25 18:54:33]
loss before attack: 15.4882
loss after attack: 18.0166
bit flips: 11
hamming_dist: 6
  **Test** Prec@1 22.160 Prec@5 100.000 Error@1 77.840
iteration Time 0.136 (0.139)
************** ATTACK iteration *****************
Iteration: [012/030]   Attack Time 0.011 (0.017)  [2024-11-25 18:54:33]
loss before attack: 18.0166
loss after attack: 20.7503
bit flips: 12
hamming_dist: 7
  **Test** Prec@1 21.020 Prec@5 100.000 Error@1 78.980
iteration Time 0.155 (0.141)
************** ATTACK iteration *****************
Iteration: [013/030]   Attack Time 0.013 (0.017)  [2024-11-25 18:54:33]
loss before attack: 20.7503
loss after attack: 23.5463
bit flips: 13
hamming_dist: 8
  **Test** Prec@1 21.020 Prec@5 100.000 Error@1 78.980
iteration Time 0.173 (0.143)
************** ATTACK iteration *****************
Iteration: [014/030]   Attack Time 0.010 (0.017)  [2024-11-25 18:54:33]
loss before attack: 23.5463
loss after attack: 26.7101
bit flips: 14
hamming_dist: 9
  **Test** Prec@1 20.720 Prec@5 100.000 Error@1 79.280
iteration Time 0.155 (0.144)
